{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the learning algorithm and implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "The Reacher environment is a environment where a double-jointed arm moves to target locations. The selecred environment was the distributed training version with 20 identical agents in the environment. The goal is to keep the hand in the target location for as long as possible, and the task can be solved by achieving an average score of +30 over 100 consecutive episodes for one agent or for all 20 agents in the distributed training version.\n",
    "The challenge of configuring the code to work with the second environment brings its advantages. They can learn with each others experiance, face less bias at the start of training as the data gathered for training is not correlated. It also allows faster data gathering, which may have lead to faster training times and potentially better performance. However it may lead to some observed varience in the beginning of training and the actions may be very exploratory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "The algorithm used for the task at hand is the \"DDPG algorithm,\" which is a hybrid of value-based and policy-based methods. The algorithm employs deep neural networks to represent the actor and critic functions. The actor selects actions to be taken in the environment, whereas the critic evaluates the actions taken by the actor and provides feedback on their quality. The aim of the task is to control a double-jointed arm to reach a target location and maintain it for as long as possible.\n",
    "\n",
    "The implementation involves several key components:\n",
    "\n",
    "1. **Actor Network:** The actor network is a neural network that takes in the current state and outputs an action. The actor is trained to maximize the Q-value predicted by the critic (mentioned below) for each state-action pair, which allows it to learn to choose actions that lead to higher Q-values.\n",
    "2. **Critic Network:** The critic network is a neural network that takes in the current state-action pair and outputs a Q-value. During training, the critic is trained using the standard TD-learning algorithm to minimize the mean squared error between the predicted Q-value and the target Q-value, which is computed using the Bellman equation.\n",
    "3. **Replay Buffer:** The replay buffer stores a history of states, actions, rewards, and next states, which are utilized for training the actor and critic networks. This buffer is valuable in breaking the correlation between consecutive state-action pairs and supplying a diverse set of experiences for the agent to learn from.\n",
    "4. **Target Networks:** To improve the stability and efficiency of the learning process, target networks are used in the DDPG algorithm. These target networks are separate copies of the actor and critic networks that are periodically updated with the weights from the online networks using a soft update strategy. By using target networks, the learning process becomes more stable and less susceptible to overfitting, which leads to faster and more reliable convergence.\n",
    "5. **Ornstein-Uhlenbeck Noise:** Ornstein-Uhlenbeck noise is a type of noise that is added to the action output by the actor network. This noise helps to explore the action space and prevents the agent from getting stuck in local optima. The Ornstein-Uhlenbeck Noise function provided for the exercises for the ddpg algorithm was changed slightly which led to a huge improvement in training. Instead of using a Python loop to generate a list of random numbers, I use use NumPy's np.random.randn function to generate an array of normally distributed random numbers. This improves the performance of the code, especially for large values of size.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome\n",
    "the DDPG algorithm was able to solve the Continuous Control environment within xxx episodes, achieving an average score of xxx over 100 consecutive episodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture of the implemented neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemented neural network contains two classes, Actor and Critic.\n",
    "\n",
    "The Actor network is responsible for learning the optimal policy that maps states to actions. It has 3 layers:\n",
    "- Input layer: receives the state of the agent as input (size 33)\n",
    "- Hidden layer 1: This is a linear layer and has 400 output neurons and uses ReLU activation function\n",
    "- Batch Normalization Layer 1: This architecture uses batch normalization layers after each the first connected layer, which can help with training stability and speed up convergence. This can help prevent vanishing or exploding gradients and make it easier to optimize the model.\n",
    "- Hidden layer 2: This is a linear layer and has 300 output neurons and uses ReLU activation function\n",
    "- Output layer: has 2 neurons and uses tanh activation function to output the corresponding action\n",
    "\n",
    "The Critic network is responsible for evaluating the quality of the actions taken by the Actor network. It has 3 layers:\n",
    "- Input layer: receives the state of the agent as input (size 33)\n",
    "- Hidden layer 1: This is a linear layer and has 400 output neurons and uses ReLU activation function\n",
    "- Batch Normalization Layer 1: This architecture uses batch normalization layers after each the first connected layer, which can help with training stability and speed up convergence. This can help prevent vanishing or exploding gradients and make it easier to optimize the model.\n",
    "- Hidden layer 2: This is a linear layer, the input comes from the first layer and is also additionally concatenated with actions of the agent (size = 4)  and has 300 output neurons and uses ReLU activation function\n",
    "- Output layer: This is a linear layer and has 1 neuron and has no activation function to output the Q-value\n",
    "\n",
    "**It was found while training that the introduction of the batch normalization layer makes a huge difference and allows faster learning.**\n",
    "\n",
    "Both networks use the hidden_init function to initialize the weights of the hidden layers to prevent vanishing or exploding gradients during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chosen hyperparameters were based on prior experience and experimentation, which showed that they lead to stable training and acceptable performance for the defined problem. They are similar to those used for the Tennis project (submitted by me before this project!).\n",
    "\n",
    "- n_episodes: The maximum number of training episodes. It is set to 2000. This hyperparameter defines the maximum number of episodes that the agent will train for before stopping. If the agent is not trained satisfactorily as many episodes it is concidered to have reached a local minima.\n",
    "\n",
    "- max_t: The maximum number of time steps per episode. It is set to 1000. This hyperparameter determines the maximum length of an episode. By limiting the number of time steps per episode, the agent can explore more episodes in a shorter amount of time. This can help speed up the learning process by allowing the agent to try out more strategies and learn from its experiences more quickly.\n",
    "\n",
    "- BUFFER_SIZE: The size of the replay buffer. In this implementation, it is set to 100,000. This hyperparameter determines the maximum number of transitions that can be stored in the replay buffer.\n",
    "\n",
    "- BATCH_SIZE: The size of the mini-batch sampled from the replay buffer for each training iteration. In this implementation, it is set to 256. This hyperparameter determines the number of transitions that are used in each update step of the neural network.\n",
    "\n",
    "- GAMMA: The discount factor used to calculate the future rewards. In this implementation, it is set to 0.99. This hyperparameter balances the importance of immediate rewards versus future rewards, with a value closer to 1.0 indicating that future rewards are given more weight.\n",
    "\n",
    "- TAU: The soft update parameter used to update the target network. In this implementation, it is set to 0.001. This hyperparameter determines the rate at which the target network is updated with the parameters of the local network, with a slower update rate typically leading to a more stable learning process.\n",
    "\n",
    "- LR_ACTOR: The learning rate used in the optimizer to update the parameters of the actor neural network. In this implementation, it is set to 0.0001. This hyperparameter determines the step size of the gradient descent during backpropagation in the actor network.\n",
    "\n",
    "- LR_CRITIC: The learning rate used in the optimizer to update the parameters of the critic neural network. In this implementation, it is set to 0.001. This hyperparameter determines the step size of the gradient descent during backpropagation in the critic network.\n",
    "\n",
    "- WEIGHT_DECAY: The L2 weight decay used in the optimizer to regularize the parameters of the neural networks. In this implementation, it is set to 0.000001. This hyperparameter adds a penalty term to the loss function to prevent the model from overfitting to the training data. Setting it to such a small value allows for faster training.\n",
    "\n",
    "- UPDATE_EVERY: This hyperparameter controls the frequency of network updates and is set to 20 in this implementation. It determines how often the agent updates its neural network based on new experiences. A higher value can lead to more stable updates, but at the cost of computational efficiency. Therefore, choosing a balance between computational efficiency and learning speed is crucial.\n",
    "\n",
    "- UPDATE_TIMES: This hyperparameter sets the number of times the neural network is updated in each training iteration, and is set to 10 in this implementation. Increasing this value can improve the stability and convergence of the learning algorithm by reducing the impact of noisy updates, but requires more computational resources. Therefore, the optimal value depends on the specific task and available resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f2f2f2; display: inline-block;\">\n",
    "     <img src=\"./Plot_of_Rewards.png\" alt=\"Plot of Rewards\" style=\"vertical-align: left;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Training_Episodes.png\" alt=\"Plot of Rewards\" style=\"vertical-align: right;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the environment was solved in xxx episodes with an average Score of xxx."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several extensions to the DDPG algorithm that can be explored to improve its performance.\n",
    "REINFORCE is a policy gradient algorithm that can be used to optimize the policy of an agent. It has been shown to be effective in tasks with high-dimensional continuous action spaces. However, it requires a large number of samples to converge and can suffer from high variance due to the use of a Monte Carlo based update rule. In comparison to DDPG, it may not be as efficient in terms of sample complexity and convergence speed. Using Generalized Advantage Estimation (GAE) alongside can also be a good direction for future work. GAE is a method for estimating the advantages of different actions taken by an agent during an episode. It uses a parameter lambda to balance the tradeoff between bias and variance in the estimation. Using GAE in the critic can help to improve the agent's ability to estimate the value of different states and actions.\n",
    "\n",
    "Trust Region Policy Optimization (TRPO) is a policy gradient algorithm that applies a trust region to restrict the policy update. It has demonstrated effectiveness in high-dimensional continuous action spaces and can achieve high sample efficiency. However, TRPO can be computationally expensive and requires careful tuning of hyperparameters.\n",
    "\n",
    "Proximal Policy Optimization (PPO) has been shown to perform well on a wide range of tasks, making it a promising alternative to DDPG for continuous control problems. It is an on-policy algorithm that combines concepts from policy gradients and trust region methods. PPO employs a surrogate function to update the policy such that the policy update is near the original policy, but still allows for substantial changes when needed. The surrogate function is intended to penalize large policy changes and promote minor changes that enhance the policy. Importance sampling, which enables us to evaluate the expected value of a function under a different distribution than the one used to generate the samples, can be employed alongside PPO to improve the stability and efficiency of the algorithm.\n",
    "\n",
    "Using other actor-critic algorithms such as A3C and A2C can be a good direction for future work. A3C (Asynchronous Advantage Actor-Critic) and A2C (Advantage Actor-Critic) are both improvements over the traditional actor-critic algorithm, with the former using multiple parallel agents and the latter using a synchronous update strategy. Both of these algorithms have shown strong performance on a variety of continuous control tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
